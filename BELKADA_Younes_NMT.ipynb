{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation - Introduction to Deep Learning Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a Neural Machine Translation model using Transformers on a translation task, using 2 types of normalization and compare their performance on the training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Imports and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -qq install sacrebleu subword-nmt sacremoses googletrans==3.1.0a0 wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from functools import partial\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import nmt_dataset\n",
    "import nnet_models\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "source_lang, target_lang = 'en', 'fr'\n",
    "model_dir = 'models/{}-{}'.format(source_lang, target_lang)\n",
    "!bash download-data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BPE Model for tokenization\n",
    "\n",
    "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. Pretokenization can be as simple as space tokenization\n",
    "\n",
    "BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size.\n",
    "\n",
    "Source: [HuggingFace](https://huggingface.co/docs/transformers/tokenizer_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "bpe_path = os.path.join(data_dir, 'bpecodes.de-en-fr')\n",
    "\n",
    "with open(bpe_path) as bpe_codes:\n",
    "    bpe_model = BPE(bpe_codes)\n",
    "\n",
    "def preprocess(line, is_source=True, source_lang=None, target_lang=None):\n",
    "    return bpe_model.segment(line.lower())\n",
    "\n",
    "def postprocess(line):\n",
    "    return line.replace('@@ ', '')\n",
    "\n",
    "def load_data(source_lang, target_lang, split='train', max_size=None):\n",
    "    # max_size: max number of sentence pairs in the training corpus (None = all)\n",
    "    path = os.path.join(data_dir, '{}.{}-{}'.format(split, *sorted([source_lang, target_lang])))\n",
    "    return nmt_dataset.load_dataset(path, source_lang, target_lang, preprocess=preprocess, max_size=max_size)   # set max_size to 10000 for fast debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the parallel corpora for the language pair\n",
    "\n",
    "The script will load a corpus and tokenize it with the BPE model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(source_lang, target_lang, 'train', max_size=10000)   # set max_size to 10000 for fast debugging\n",
    "valid_data = load_data(source_lang, target_lang, 'valid')\n",
    "test_data = load_data(source_lang, target_lang, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_data</th>\n",
       "      <th>target_data</th>\n",
       "      <th>source_tokenized</th>\n",
       "      <th>target_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all tom needed was time .</td>\n",
       "      <td>tout ce que tom avait besoin , c' était du tem...</td>\n",
       "      <td>[all, tom, needed, was, time, .]</td>\n",
       "      <td>[tout, ce, que, tom, avait, besoin, ,, c', éta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i 'm waiting for my friend .</td>\n",
       "      <td>j' attends mon amie .</td>\n",
       "      <td>[i, 'm, waiting, for, my, friend, .]</td>\n",
       "      <td>[j', attends, mon, amie, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why do you need a knife ?</td>\n",
       "      <td>pourquoi as-tu besoin d' un couteau ?</td>\n",
       "      <td>[why, do, you, need, a, knife, ?]</td>\n",
       "      <td>[pourquoi, as-tu, besoin, d', un, couteau, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he lives off the grid .</td>\n",
       "      <td>il vit en marge de la société .</td>\n",
       "      <td>[he, lives, off, the, gri@@, d, .]</td>\n",
       "      <td>[il, vit, en, mar@@, ge, de, la, société, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the bridge is built of wood .</td>\n",
       "      <td>ce pont est fait en bois .</td>\n",
       "      <td>[the, bridge, is, built, of, wo@@, od, .]</td>\n",
       "      <td>[ce, pont, est, fait, en, bois, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     source_data  \\\n",
       "0      all tom needed was time .   \n",
       "1   i 'm waiting for my friend .   \n",
       "2      why do you need a knife ?   \n",
       "3        he lives off the grid .   \n",
       "4  the bridge is built of wood .   \n",
       "\n",
       "                                         target_data  \\\n",
       "0  tout ce que tom avait besoin , c' était du tem...   \n",
       "1                              j' attends mon amie .   \n",
       "2              pourquoi as-tu besoin d' un couteau ?   \n",
       "3                    il vit en marge de la société .   \n",
       "4                         ce pont est fait en bois .   \n",
       "\n",
       "                            source_tokenized  \\\n",
       "0           [all, tom, needed, was, time, .]   \n",
       "1       [i, 'm, waiting, for, my, friend, .]   \n",
       "2          [why, do, you, need, a, knife, ?]   \n",
       "3         [he, lives, off, the, gri@@, d, .]   \n",
       "4  [the, bridge, is, built, of, wo@@, od, .]   \n",
       "\n",
       "                                    target_tokenized  \n",
       "0  [tout, ce, que, tom, avait, besoin, ,, c', éta...  \n",
       "1                        [j', attends, mon, amie, .]  \n",
       "2      [pourquoi, as-tu, besoin, d', un, couteau, ?]  \n",
       "3       [il, vit, en, mar@@, ge, de, la, société, .]  \n",
       "4                 [ce, pont, est, fait, en, bois, .]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why you have the symbol @?? À explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dict_path = os.path.join(model_dir, 'dict.{}.txt'.format(source_lang))\n",
    "target_dict_path = os.path.join(model_dir, 'dict.{}.txt'.format(target_lang))\n",
    "\n",
    "source_dict = nmt_dataset.load_or_create_dictionary(\n",
    "    source_dict_path,\n",
    "    train_data['source_tokenized'],\n",
    "    minimum_count=10,\n",
    "    reset=False    # set reset to True if you're changing the data or the preprocessing\n",
    ")\n",
    "\n",
    "target_dict = nmt_dataset.load_or_create_dictionary(\n",
    "    target_dict_path,\n",
    "    train_data['target_tokenized'],\n",
    "    minimum_count=10,\n",
    "    reset=False\n",
    ")\n",
    "\n",
    "multi_model_dir = os.path.join('pretrained_models', 'de-en-fr')\n",
    "\n",
    "multi_dict = nmt_dataset.load_or_create_dictionary(\n",
    "        os.path.join(multi_model_dir, 'dict.txt'),\n",
    "        dataset=None,\n",
    "        minimum_count=10,\n",
    "        reset=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the dictionary to map each token into a binarized token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_dataset.binarize(train_data, multi_dict, multi_dict, sort=True)\n",
    "nmt_dataset.binarize(valid_data, multi_dict, multi_dict, sort=False)\n",
    "nmt_dataset.binarize(test_data, multi_dict, multi_dict, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build the batch dataset\n",
    "\n",
    "Here we create our dataset that consists of a batch of sequence of words of the same lengths in order to be fed to our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30       # maximum 30 tokens per sentence (longer sequences will be truncated)\n",
    "batch_size = 512   # maximum 512 tokens per batch (decrease if you get OOM errors, increase to speed up training)\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "# *****START CODE\n",
    "train_iterator = nmt_dataset.BatchIterator(train_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=True)\n",
    "valid_iterator = nmt_dataset.BatchIterator(valid_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=False)\n",
    "test_iterator = nmt_dataset.BatchIterator(test_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=False)\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "Here we will create **4** models:\n",
    "+ A shallow model with the residual normalization \n",
    "+ A shallow model with the classic normalization\n",
    "+ A deep model with the classic normalization\n",
    "+ A deep model with the classic normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_transformer_encoder_preLN = nnet_models.TransformerEncoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    heads=4,\n",
    "    normalize_before = True\n",
    ")\n",
    "shallow_transformer_decoder_preLN = nnet_models.TransformerDecoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    heads=4,\n",
    "    dropout=0.0,\n",
    "    normalize_before = True\n",
    ")\n",
    "\n",
    "shallow_transformer_model_preLN = nnet_models.EncoderDecoder(\n",
    "    shallow_transformer_encoder_preLN,\n",
    "    shallow_transformer_decoder_preLN,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, checkpoint_path):\n",
    "    dirname = os.path.dirname(checkpoint_path)\n",
    "    if dirname:\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "    torch.save(model, checkpoint_path)\n",
    "\n",
    "def train_model(\n",
    "        train_iterator,\n",
    "        valid_iterators,\n",
    "        model,\n",
    "        checkpoint_path,\n",
    "        epochs=1,\n",
    "        validation_frequency=1\n",
    "    ):\n",
    "    \"\"\"\n",
    "    train_iterator: instance of nmt_dataset.BatchIterator or nmt_dataset.MultiBatchIterator\n",
    "    valid_iterators: list of nmt_dataset.BatchIterator\n",
    "    model: instance of nnet_models.EncoderDecoder\n",
    "    checkpoint_path: path of the model checkpoint\n",
    "    epochs: iterate this many times over train_iterator\n",
    "    validation_frequency: validate the model every N epochs\n",
    "    \"\"\"\n",
    "\n",
    "    reset_seed()\n",
    "\n",
    "    best_bleu = -1\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        print('Epoch: [{}/{}]'.format(epoch, epochs))\n",
    "\n",
    "        # Iterate over training batches for one epoch\n",
    "        for i, batch in tqdm(enumerate(train_iterator), total=len(train_iterator)):\n",
    "            t = time.time()\n",
    "            running_loss += model.train_step(batch)\n",
    "\n",
    "        # Average training loss for this epoch\n",
    "        # *****START CODE\n",
    "        epoch_loss = running_loss / len(train_iterator)\n",
    "        # *****END CODE\n",
    "\n",
    "        print(\"loss={:.3f}, time={:.2f}\".format(epoch_loss, time.time() - start))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Evaluate and save the model\n",
    "        if epoch % validation_frequency == 0:\n",
    "            bleu_scores = []\n",
    "            \n",
    "            # Compute BLEU over all validation sets\n",
    "            for valid_iterator in valid_iterators:\n",
    "                # *****START CODE\n",
    "                src, tgt = valid_iterator.source_lang, valid_iterator.target_lang\n",
    "                translation_output = model.translate(valid_iterator, postprocess)\n",
    "                bleu_score = translation_output.score\n",
    "                output = translation_output.output\n",
    "                # *****END CODE\n",
    "\n",
    "                with open(os.path.join(model_dir, 'valid.{}-{}.{}.out'.format(src, tgt, epoch)), 'w') as f:\n",
    "                    f.writelines(line + '\\n' for line in output)\n",
    "\n",
    "                print('{}-{}: BLEU={}'.format(src, tgt, bleu_score))\n",
    "                sys.stdout.flush()\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "            # Average the validation BLEU scores\n",
    "            bleu_score = round(sum(bleu_scores) / len(bleu_scores), 2)\n",
    "            if len(bleu_scores) > 1:\n",
    "                print('BLEU={}'.format(bleu_score))\n",
    "\n",
    "            # Update the model's learning rate based on current performance.\n",
    "            # This scheduler divides the learning rate by 10 if BLEU does not improve.\n",
    "            model.scheduler_step(bleu_score)\n",
    "\n",
    "            # Save a model checkpoint if it has the best validation BLEU so far\n",
    "            if bleu_score > best_bleu:\n",
    "                best_bleu = bleu_score\n",
    "                save_model(model, checkpoint_path)\n",
    "\n",
    "        print('=' * 50)\n",
    "\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20465 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-47f32524892b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_model(train_iterator, [valid_iterator], shallow_transformer_model_preLN,\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 checkpoint_path='models')\n",
      "\u001b[0;32m<ipython-input-24-81b14f047e9c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_iterator, valid_iterators, model, checkpoint_path, epochs, validation_frequency)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Average training loss for this epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Travail/MVA/DeepLearning/NMT_DL/nnet_models.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mencoder_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxs_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_hidden'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Travail/MVA/DeepLearning/NMT_DL/nnet_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, xs_len, lang)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mself_attn_weights_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             x, self_attn_weights = layer(\n\u001b[0m\u001b[1;32m    615\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                 \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Travail/MVA/DeepLearning/NMT_DL/nnet_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         x, self_attn_weights_all_heads = self.self_attn(\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             key_padding_mask=src_key_padding_mask)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 v_proj_weight=self.v_proj_weight)\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_model(train_iterator, [valid_iterator], shallow_transformer_model_preLN,\n",
    "                epochs=1,\n",
    "                checkpoint_path='models')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "963f4a9dc41c9d81e56e3cb1fd0b74163d55b45a806c0e1378beb0edaea0e776"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv-project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
